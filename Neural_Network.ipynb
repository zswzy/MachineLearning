{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import random\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    reading finished\n",
      "\n",
      "processing data...\n",
      "    processing finished\n"
     ]
    }
   ],
   "source": [
    "#nreading data\n",
    "Data = pd.read_csv('train_V2.csv')\n",
    "Data.head(10)\n",
    "Data=Data.round({'winPlacePerc':3})\n",
    "Data = Data.dropna(axis = 0)\n",
    "data = Data.drop(['Id','groupId','matchId'],axis = 1)\n",
    "data = data[(Data['matchType'] == ('solo' or 'solo-fpp'))]\n",
    "data = data.drop(['matchType'],axis = 1) #最终清洗之后的数据集\n",
    "print('    reading finished')\n",
    "\n",
    "print('\\nprocessing data...')\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(data.drop(['winPlacePerc'],axis = 1),data['winPlacePerc'],test_size=0.4)\n",
    "\n",
    "X_Train=X_train.values\n",
    "Y_Train=Y_train.values*10000\n",
    "print('    processing finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934\n"
     ]
    }
   ],
   "source": [
    "print(len(np.unique(Y_Train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar=StandardScaler()\n",
    "scalar.fit(X_Train)\n",
    "X_Train=scalar.transform(X_Train)\n",
    "scalar=StandardScaler()\n",
    "scalar.fit(X_test.values)\n",
    "X_Test=scalar.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "print(len(X_Train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unroll_parameters, transform a list of weight matrices into an 1D array\n",
    "def unroll_params(Theta):\n",
    "    nn_params = np.reshape(Theta[0], (1, -1)).transpose()\n",
    "    for i in range(1, len(Theta)):\n",
    "        nn_params = np.concatenate((nn_params, np.reshape(Theta[i], (1, -1)).transpose()))\n",
    "\n",
    "    nn_params = np.ndarray.flatten(nn_params)\n",
    "\n",
    "    return nn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roll_params, transform a list of weights in to an list of weight matrixes\n",
    "def roll_params(nn_params, layers):\n",
    "    # Setup some useful variables\n",
    "    num_layers = len(layers)\n",
    "    Theta = []\n",
    "    index = 0\n",
    "    for i in range(num_layers - 1):\n",
    "        step = layers[i + 1] * (layers[i] + 1)\n",
    "        Theta.append(np.reshape(nn_params[index:(index + step)], (layers[i + 1], (layers[i] + 1))))\n",
    "\n",
    "        index = index + step\n",
    "\n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialyze the weights\n",
    "def randInitializeWeights(layers):\n",
    "    num_of_layers = len(layers)\n",
    "    epsilon = 1\n",
    "\n",
    "    Theta = []\n",
    "    for i in range(num_of_layers - 1):\n",
    "        W = np.zeros((layers[i + 1], layers[i] + 1), dtype='float64')\n",
    "        for m in range(len(W)):\n",
    "            for n in range(len(W[0])):\n",
    "                W[m][n] = 2 * epsilon * random() - epsilon\n",
    "        Theta.append(W)\n",
    "\n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized CostFunction of the neural network\n",
    "def costFunction(nn_weights, layers, X, y, num_labels, l):\n",
    "    # Setup some useful variables\n",
    "    m = X.shape[0]\n",
    "    num_layers = len(layers)\n",
    "\n",
    "    # Unroll Params\n",
    "    Theta = roll_params(nn_weights, layers)\n",
    "    J = 0\n",
    "    yv = np.zeros((num_labels, m))\n",
    "    for i in range(m):\n",
    "        yv[int(y[i]), i] = 1.0\n",
    "\n",
    "    # feedforward\n",
    "    activation = np.transpose(np.concatenate((np.ones((m, 1)), X), axis=1))\n",
    "    activations = [activation]\n",
    "    zs = []  # list to store all the z vectors, layer by layer\n",
    "    for i in range(num_layers - 1):\n",
    "        z = np.dot(Theta[i], activation)\n",
    "        zs.append(z)\n",
    "        if i == (num_layers - 2):  # Final layer\n",
    "            activation = sigmoid(z)\n",
    "        else:\n",
    "            activation = np.concatenate((np.ones((1, m)), sigmoid(z)), axis=0)\n",
    "\n",
    "        activations.append(activation)\n",
    "        # Cost Function\n",
    "    J = (1.0 / m) * (np.sum(-1.0 * yv * np.log(activations[-1]) - (1.0 - yv) * np.log(1.0 - activations[-1])))\n",
    "    for i in range(num_layers - 1):\n",
    "        J = J + (l / (2.0 * m)) * np.sum(pow(Theta[i][:, 1:], 2.0))\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(z):\n",
    "\n",
    "    # SIGMOID returns sigmoid function evaluated at z\n",
    "    g = np.zeros(np.shape(z))\n",
    "    g=1/(1+np.exp(-z))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid gradient\n",
    "def sigmoidGradient(z):\n",
    "\n",
    "    g=np.ones(np.shape(z))\n",
    "    #calculated the gradient for all shapes of z\n",
    "    if np.shape(z)==():\n",
    "        g=sigmoid(z)*(1-sigmoid(z))\n",
    "    elif np.size(z)==len(z):\n",
    "        for i in range (len(z)):\n",
    "            g[i]=sigmoid(z[i])*(1-sigmoid(z[i]))\n",
    "    else:\n",
    "        for i in range (len(z)):\n",
    "            for j in range (len(z[0])):\n",
    "                g[i][j]=sigmoid(z[i][j])*(1-sigmoid(z[i][j]))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the weights to predict the output of the neural network\n",
    "def predict(Theta, X):\n",
    "\n",
    "    # Useful values\n",
    "    m = X.shape[0]\n",
    "    num_labels = Theta[-1].shape[0]\n",
    "    num_layers = len(Theta) + 1\n",
    "\n",
    "    p = np.zeros((1, m))\n",
    "    h = X\n",
    "    activation = np.transpose(np.concatenate((np.ones((m, 1)), X), axis=1))\n",
    "    for i in range(num_layers - 1):\n",
    "        z = np.dot(Theta[i], activation)\n",
    "        if i == (num_layers - 2):\n",
    "            activation = sigmoid(z)\n",
    "        else:\n",
    "            activation = np.concatenate((np.ones((1, m)), sigmoid(z)), axis=0)\n",
    "    p = np.argmax(activation, axis=0)\n",
    "\n",
    "    return (p/100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwards(nn_weights, layers, X, y, num_labels, lambd):\n",
    "    # nn_weights: Neural network parameters (vector)\n",
    "    # layers: a list with the number of units per layer.\n",
    "    # X: a matrix where every row is a training example for a handwritten digit image\n",
    "    # y: a vector with the labels of each instance\n",
    "    # num_labels: the number of units in the output layer\n",
    "    # lambd: regularization factor\n",
    "    # Setup some useful variables\n",
    "    m = X.shape[0]\n",
    "    num_layers = len(layers)\n",
    "\n",
    "    # Roll Params\n",
    "    # The parameters for the neural network are \"unrolled\" into the vector\n",
    "    # nn_params and need to be converted back into the weight matrices.\n",
    "    Theta = roll_params(nn_weights, layers)\n",
    "\n",
    "    # You need to return the following variables correctly\n",
    "    Theta_grad = [np.zeros(w.shape) for w in Theta]\n",
    "\n",
    "    yv = np.zeros((num_labels, m))\n",
    "    for i in range(m):\n",
    "        yv[int(y[i]), i] = 1\n",
    "\n",
    "    activation = np.transpose(np.concatenate((np.ones((m, 1)), X), axis=1))\n",
    "    activations = [activation]\n",
    "    zs = []  # list to store all the z vectors, layer by layer\n",
    "    for i in range(num_layers - 1):\n",
    "        z = np.dot(Theta[i], activation)\n",
    "        zs.append(z)\n",
    "        if i == (num_layers - 2):  # Final layer\n",
    "            activation = sigmoid(z)\n",
    "        else:\n",
    "            activation = np.concatenate((np.ones((1, m)), sigmoid(z)), axis=0)\n",
    "\n",
    "        activations.append(activation)\n",
    "\n",
    "    # backward pass\n",
    "    delta = activations[-1] - yv\n",
    "    Theta_grad[-1] = (1.0 / m) * np.dot(delta, activations[-2].transpose())\n",
    "    Theta_grad[-1][:, 1:] = Theta_grad[-1][:, 1:] + (lambd / m) * Theta[-1][:, 1:]\n",
    "    for l in range(2, num_layers):\n",
    "        delta = np.dot(Theta[-l + 1].transpose(), delta)[1:, :] * sigmoidGradient(zs[-l])\n",
    "        Theta_grad[-l] = (1.0 / m) * np.dot(delta, activations[-l - 1].transpose())\n",
    "        Theta_grad[-l][:, 1:] = Theta_grad[-l][:, 1:] + (lambd / m) * (Theta[-l][:, 1:])\n",
    "\n",
    "    # Unroll Params\n",
    "    Theta_grad = unroll_params(Theta_grad)\n",
    "\n",
    "    return Theta_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NN(layers,X_Train,Y_Train,X_Test):\n",
    "\n",
    "    print(\"\\nSetting up Neural Network Structure ...\\n\")\n",
    "\n",
    "    input_layer_size = len(X_Train[0])\n",
    "    num_labels = 101  # 101 labels, from 0 to 1.00\n",
    "\n",
    "    print(\"\\nInitializing Neural Network Parameters ...\\n\")\n",
    "    Theta = randInitializeWeights(layers)\n",
    "    # Unroll parameters\n",
    "    nn_weights = unroll_params(Theta)\n",
    "\n",
    "    print(\"\\nTraining Neural Network... \\n\" )\n",
    "\n",
    "    # We can change the regularized factor\n",
    "    lambd = 0\n",
    "    #train the model\n",
    "    res = fmin_l_bfgs_b(costFunction, nn_weights, fprime=backwards, args=(layers, X_Train, Y_Train, num_labels, lambd),\n",
    "                        maxfun=50, factr=1., disp=True)\n",
    "    Theta = roll_params(res[0], layers)\n",
    "\n",
    "    #nn_weights = unroll_params(Theta)\n",
    "\n",
    "    print(\"\\nPredictinf results... \\n\")\n",
    "\n",
    "    pred = predict(Theta, X_test)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up Neural Network Structure ...\n",
      "\n",
      "\n",
      "Initializing Neural Network Parameters ...\n",
      "\n",
      "\n",
      "Training Neural Network... \n",
      "\n",
      "\n",
      "Predictinf results... \n",
      "\n",
      "The mean squared error is : 0.040321665292662824\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "layers=[24,30,101]\n",
    "pred=train_NN(layers,X_Train,Y_Train,X_Test)\n",
    "Y_pred=[]\n",
    "for i in range (len(pred)):\n",
    "    Y_pred.append(float(pred[i]))\n",
    "print('The mean squared error is : %s'%(mean_squared_error(Y_test,Y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
